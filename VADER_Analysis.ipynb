{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKH0yg_pfh-8",
        "outputId": "58b05d06-da6e-48df-f5ea-246a8b35ec4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk import word_tokenize\n",
        "import requests\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import statsmodels.api as sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3qQY4v70BKk"
      },
      "source": [
        "Sentiment Vader Anlaysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RsMl_oWsev8T"
      },
      "outputs": [],
      "source": [
        "def clean_tweet(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    lowercase_words = [word.lower() for word in words\n",
        "                       if word not in stopwords.words() and word.isalpha()]\n",
        "    temp_text = ' '.join(lowercase_words)\n",
        "    return temp_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hrlAL9QpfIIk"
      },
      "outputs": [],
      "source": [
        "def vader_comparison(sentence):\n",
        "    sentence=clean_tweet(sentence)\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    pos=compound=neu=neg=0\n",
        "    vs = analyzer.polarity_scores(sentence)\n",
        "    pos = vs['pos']\n",
        "    neu = vs['neu']\n",
        "    neg = vs['neg']\n",
        "    compound = vs['compound']\n",
        "    if (compound > 0):\n",
        "        return 1\n",
        "    elif (compound == 0):\n",
        "        return 0\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to run VADER sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X9Q7oC0vtqrm"
      },
      "outputs": [],
      "source": [
        "def process_chunk(chunk):\n",
        "    for index, row in chunk.iterrows():\n",
        "        try:\n",
        "            text = str(row['text']) if pd.notna(row['text']) else ''\n",
        "            # Apply sentiment analysis to the text in the current row\n",
        "            sentiment_score = vader_comparison(row['text'])\n",
        "            # Store the sentiment score in a new column\n",
        "            chunk.at[index, 'sentiment_vader'] = sentiment_score\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {index}: {e}\")\n",
        "            print(\"Text that caused the error:\", row['text'])\n",
        "    return chunk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to read the dataset in chunks as the Tweet dataset is huge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qKxkqZjjttj-"
      },
      "outputs": [],
      "source": [
        "# Function to read the dataset in chunks and process each chunk\n",
        "def process_dataset_in_chunks(dataset_path,  start_chunk_index, chunk_size=1000):\n",
        "    chunk_iterator = pd.read_csv(dataset_path, chunksize=chunk_size, lineterminator = '\\n')\n",
        "    for i, chunk in enumerate(chunk_iterator):\n",
        "        if i < start_chunk_index:\n",
        "            continue  # Skip chunks until reaching the start_chunk_index\n",
        "\n",
        "        print(f\"Processing chunk {i+1}\")\n",
        "        processed_chunk = process_chunk(chunk)\n",
        "        output_file = fr\"C:\\Users\\user\\OneDrive\\Desktop\\Research Project\\Chunk Folder\\processed_chunk_{i+1}.csv\"\n",
        "        processed_chunk.to_csv(output_file, index=False)\n",
        "        print(f\"Chunk {i+1} processed and saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g3IMjhORw5Mg"
      },
      "outputs": [],
      "source": [
        "dataset_path = r\"C:\\Users\\user\\OneDrive\\Desktop\\Research Project\\Codes\\bitcoin-tweets-2021.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkrFMSI2ZDbf",
        "outputId": "24bcee28-8358-4dfa-f35e-c2340ce3c459"
      },
      "outputs": [],
      "source": [
        "start_chunk_index = 8000\n",
        "process_dataset_in_chunks(dataset_path, start_chunk_index)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
